{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translate English text to python code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjHBAWF12k/lFhx8TXfbOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikshrimali/ENDGAME_MERGER/blob/main/CapStone/Translate_English_text_to_python_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGG7-Ga5V0f0"
      },
      "source": [
        "# Transformer based model to translate English text to Python code\r\n",
        "\r\n",
        "The goal is to  write a transformer-based model that can translats English text to python code(with proper whitespace indentations)\r\n",
        "\r\n",
        "The training dataset contains around 4600+ examples of English text to python code. \r\n",
        "- must use transformers with self-attention, multi-head, and scaled-dot product attention in the model\r\n",
        "- There is no limit on the number of training epochs or total number of parameters in the model\r\n",
        "- should have trained a separate embedding layer for python keywords and paid special attention to whitespaces, colon and other things (like comma etc)\r\n",
        "- model should to do proper indentation\r\n",
        "- model should to use newline properly\r\n",
        "- model should understand how to use colon (:)\r\n",
        "- model should generate proper python code that can run on a Python interpreter and produce proper results\r\n",
        "\r\n",
        "\r\n",
        "Some preprocessing checks on the dataset should be carried out like:\r\n",
        " - the dataset provided is divided into English and \"python-code\" pairs properly\r\n",
        "the dataset does not have anomalies w.r.t. indentations (like a mixed-use of tabs and spaces, or use of either 4 or 3 spaces, it should be 4 spaces only). Either use tabs only or 4 spaces only, not both\r\n",
        "- the length of the \"python-code\" generated is not out of your model's capacity\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj3-MKeYhqlM"
      },
      "source": [
        "import torch\r\n",
        "from torch.jit import script, trace\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import csv\r\n",
        "import random\r\n",
        "import re\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "import codecs\r\n",
        "from io import open\r\n",
        "import itertools\r\n",
        "import math\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from torchtext.data import Field, BucketIterator, LabelField, TabularDataset\r\n",
        "\r\n",
        "\r\n",
        "USE_CUDA = torch.cuda.is_available()\r\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7K58TvqsZfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f5aefe-4338-42cc-99b5-37f891386688"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7WcRrmo1BM7"
      },
      "source": [
        "datasets = [[]]\r\n",
        "file_name = '/content/drive/MyDrive/english_python_data_cleaned.txt'\r\n",
        "\r\n",
        "with open(file_name) as f:\r\n",
        "  #my_dict = {\"description\":[],\"code\":[]}\r\n",
        "  for line in f:\r\n",
        "    if line.startswith('#'):\r\n",
        "      comment = line.split('\\n#')\r\n",
        "      if datasets[-1] != []:\r\n",
        "        # we are in a new block\r\n",
        "        datasets.append(comment)\r\n",
        "    else:\r\n",
        "      stripped_line = line#.strip()\r\n",
        "      if stripped_line:\r\n",
        "        datasets[-1].append(stripped_line)\r\n",
        "# datasets[0].insert(0,'# write a python program to add two numbers ')        "
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwhevQBOTPoc"
      },
      "source": [
        "raw_data = {'Description' : [re.sub(r\"^#(\\d)*\",'',x[0]).strip() for x in datasets], 'Code': [''.join(x[1:]) for x in datasets]}\r\n",
        "df = pd.DataFrame(raw_data, columns=[\"Description\", \"Code\"])"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P64W2zTDcUe-",
        "outputId": "5ee62fbf-654f-47c1-96e7-045e9cea67ca"
      },
      "source": [
        "df['Description'][1617]"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'. write a python program to subtract two numbers'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "xxXXhkonTg8q",
        "outputId": "0c33d883-27b8-4a1e-fc94-cabfd0e8720a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>write a python program to add two numbers</td>\n",
              "      <td>\\nnum1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\np...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>write a python function to add two user provid...</td>\n",
              "      <td>def add_two_numbers(num1, num2):\\n    sum = nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>write a program to find and print the largest ...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &gt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>write a program to find and print the smallest...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &lt;= n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Description                                               Code\n",
              "0                                                                                                      \n",
              "1          write a python program to add two numbers  \\nnum1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\np...\n",
              "2  write a python function to add two user provid...  def add_two_numbers(num1, num2):\\n    sum = nu...\n",
              "3  write a program to find and print the largest ...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 >= n...\n",
              "4  write a program to find and print the smallest...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <= n..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlaGjIFLeh99"
      },
      "source": [
        "df['Code'].replace(\"\", float(\"NaN\"), inplace=True)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "XnqkdzWbg2H-",
        "outputId": "f101d31b-f48c-46d0-8ac6-320508a8ea1e"
      },
      "source": [
        "df[df.isna().any(axis=1)]"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Description Code\n",
              "0              NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxQVB1MEgZZQ"
      },
      "source": [
        "df.dropna(subset = [\"Code\"], inplace=True)\r\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWWDmOTDQYhf",
        "outputId": "fa24e3be-e26c-4654-aab1-d0ce01dcecda"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4473 entries, 1 to 4473\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Description  4473 non-null   object\n",
            " 1   Code         4473 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 104.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OyrtWssHCKh"
      },
      "source": [
        "\r\n",
        "# Dividing the data into train and validation dataset\r\n",
        "\r\n",
        "train_df = df.sample(frac = 0.80) \r\n",
        "  \r\n",
        "# Creating dataframe with rest of the 20% values \r\n",
        "valid_df = df.drop(train_df.index)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts-QRvzPdLKb",
        "outputId": "c569f980-2912-4809-aba7-bf7df0cd2e8b"
      },
      "source": [
        "print(f'train df {train_df}')\r\n",
        "print(f'Valid df {valid_df}')\r\n",
        "\r\n",
        "train_df.to_csv('train.csv', index=False)\r\n",
        "valid_df.to_csv('valid.csv', index=False)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train df                                             Description                                               Code\n",
            "716   Don't use mutable objects as default arguments...  def append_to_list(value, def_list=[]):\\n    d...\n",
            "3961   write a python program to merge two dictionaries  \\n\\ndict_1 = {'apple': 9, 'banana': 6}\\ndict_2...\n",
            "2837  write a program to print the product of intege...  test_list = [5, 8, \"gfg\", 8, (5, 7), 'is', 2] ...\n",
            "2894  write a python function that would print the A...  def print_ascii(char):\\n    print(ord(char))\\n...\n",
            "688    function to showcast documemtation of namedtuple  def show_doc_named():\\n    from collections im...\n",
            "...                                                 ...                                                ...\n",
            "2204  write a function to check weather a number is ...  \\n\\ndef isprime(num):\\n    for i in range(2, n...\n",
            "294   write a python function to prepend a single va...  def prepend(value, iterator):    \\n    import ...\n",
            "4304  write Python3 program for illustration of valu...  \\n\\ndictionary = {\"raj\": 2, \"striver\": 3, \"vik...\n",
            "1931  47. python function for finding the exponent o...  def exp(x):\\n\\n\\n  \"\"\"returns e^x of a number\"...\n",
            "1119  Write a Python function that Given a string, d...  def printEveIndexChar(str):\\n  for i in range(...\n",
            "\n",
            "[3578 rows x 2 columns]\n",
            "Valid df                                             Description                                               Code\n",
            "14    Write a program to swap first and last element...  my_list = [1, 2, 3, 4, 5, 6]\\nmy_list[0], my_l...\n",
            "21    Write a python function to remove all the even...  def remove_even(my_list):\\n    result = list(f...\n",
            "22    Write a function that takes two lists as input...  def zip_list(list1, list2):\\n    return list(z...\n",
            "25    Write a program to print the unique elements i...  \\n\\nmy_list = [1, 2, 4, 5, 2, 3, 1, 5, 4, 7, 8...\n",
            "32    Write a function to find the perimeter of a re...  \\n\\ndef rectangle_perimeter(l, b):\\n    return...\n",
            "...                                                 ...                                                ...\n",
            "4441    write a program to count characters in a string  st = \"AmmarAdil\"\\ncount = {}\\nfor a in st:\\n  ...\n",
            "4448              write a program to create zero matrix  rows = 2\\ncols = 3\\nM = []\\nwhile len(M) < row...\n",
            "4461  write a program to find absoluute number of a ...  import math\\nnumber = 34.564\\nfa = math.fabs(n...\n",
            "4465         write a program to find cosine of a number  import math\\nnum = 45\\nprint(\"Cosine\", math.co...\n",
            "4472      write a program to Binary Left Shift a number      c = a << 2\\nprint(\"Binary Left Shift\", c)\\n\\n\n",
            "\n",
            "[895 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ze0bFFIWyQP"
      },
      "source": [
        "# import io\r\n",
        "# from io import BytesIO\r\n",
        "# from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP, tok_name\r\n",
        "\r\n",
        "# def tokenize_code(text):\r\n",
        "#     result = []\r\n",
        "#     for tok in tokenize(io.BytesIO(text.encode('utf-8')).readline):\r\n",
        "#         if tok_name[tok.exact_type] == 'NAME':\r\n",
        "#             result.append(tok.string)\r\n",
        "#         else:\r\n",
        "#             result.append(tok_name[tok.exact_type])\r\n",
        "#     return result"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQE5bFuMyIwe",
        "outputId": "c2e17fc6-611d-44fc-b77c-5027d7fd2a71"
      },
      "source": [
        "# tokenize_code(df['Code'][1])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ENCODING',\n",
              " 'NL',\n",
              " 'num1',\n",
              " 'EQUAL',\n",
              " 'NUMBER',\n",
              " 'NEWLINE',\n",
              " 'num2',\n",
              " 'EQUAL',\n",
              " 'NUMBER',\n",
              " 'NEWLINE',\n",
              " 'sum',\n",
              " 'EQUAL',\n",
              " 'num1',\n",
              " 'PLUS',\n",
              " 'num2',\n",
              " 'NEWLINE',\n",
              " 'print',\n",
              " 'LPAR',\n",
              " 'STRING',\n",
              " 'RPAR',\n",
              " 'NEWLINE',\n",
              " 'NL',\n",
              " 'ENDMARKER']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIbH2-jX8P1l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "43da27dc-bd14-4525-b358-9a5a7d7f9e0f"
      },
      "source": [
        "'''\r\n",
        "ENDMARKER = 0\r\n",
        "NAME = 1\r\n",
        "NUMBER = 2\r\n",
        "STRING = 3\r\n",
        "NEWLINE = 4\r\n",
        "INDENT = 5\r\n",
        "DEDENT = 6\r\n",
        "LPAR = 7\r\n",
        "RPAR = 8\r\n",
        "LSQB = 9\r\n",
        "RSQB = 10\r\n",
        "COLON = 11\r\n",
        "COMMA = 12\r\n",
        "SEMI = 13\r\n",
        "PLUS = 14\r\n",
        "MINUS = 15\r\n",
        "STAR = 16\r\n",
        "SLASH = 17\r\n",
        "VBAR = 18\r\n",
        "AMPER = 19\r\n",
        "LESS = 20\r\n",
        "GREATER = 21\r\n",
        "EQUAL = 22\r\n",
        "DOT = 23\r\n",
        "PERCENT = 24\r\n",
        "LBRACE = 25\r\n",
        "RBRACE = 26\r\n",
        "EQEQUAL = 27\r\n",
        "NOTEQUAL = 28\r\n",
        "LESSEQUAL = 29\r\n",
        "GREATEREQUAL = 30\r\n",
        "TILDE = 31\r\n",
        "CIRCUMFLEX = 32\r\n",
        "LEFTSHIFT = 33\r\n",
        "RIGHTSHIFT = 34\r\n",
        "DOUBLESTAR = 35\r\n",
        "PLUSEQUAL = 36\r\n",
        "MINEQUAL = 37\r\n",
        "STAREQUAL = 38\r\n",
        "SLASHEQUAL = 39\r\n",
        "PERCENTEQUAL = 40\r\n",
        "AMPEREQUAL = 41\r\n",
        "VBAREQUAL = 42\r\n",
        "CIRCUMFLEXEQUAL = 43\r\n",
        "LEFTSHIFTEQUAL = 44\r\n",
        "RIGHTSHIFTEQUAL = 45\r\n",
        "DOUBLESTAREQUAL = 46\r\n",
        "DOUBLESLASH = 47\r\n",
        "DOUBLESLASHEQUAL = 48\r\n",
        "AT = 49\r\n",
        "ATEQUAL = 50\r\n",
        "RARROW = 51\r\n",
        "ELLIPSIS = 52\r\n",
        "COLONEQUAL = 53\r\n",
        "OP = 54\r\n",
        "AWAIT = 55\r\n",
        "ASYNC = 56\r\n",
        "TYPE_IGNORE = 57\r\n",
        "TYPE_COMMENT = 58\r\n",
        "# These aren't used by the C tokenizer but are needed for tokenize.py\r\n",
        "ERRORTOKEN = 59\r\n",
        "COMMENT = 60\r\n",
        "NL = 61\r\n",
        "ENCODING = 62\r\n",
        "N_TOKENS = 63\r\n",
        "# Special definitions for cooperation with parser\r\n",
        "NT_OFFSET = 256\r\n",
        "'''"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nENDMARKER = 0\\nNAME = 1\\nNUMBER = 2\\nSTRING = 3\\nNEWLINE = 4\\nINDENT = 5\\nDEDENT = 6\\nLPAR = 7\\nRPAR = 8\\nLSQB = 9\\nRSQB = 10\\nCOLON = 11\\nCOMMA = 12\\nSEMI = 13\\nPLUS = 14\\nMINUS = 15\\nSTAR = 16\\nSLASH = 17\\nVBAR = 18\\nAMPER = 19\\nLESS = 20\\nGREATER = 21\\nEQUAL = 22\\nDOT = 23\\nPERCENT = 24\\nLBRACE = 25\\nRBRACE = 26\\nEQEQUAL = 27\\nNOTEQUAL = 28\\nLESSEQUAL = 29\\nGREATEREQUAL = 30\\nTILDE = 31\\nCIRCUMFLEX = 32\\nLEFTSHIFT = 33\\nRIGHTSHIFT = 34\\nDOUBLESTAR = 35\\nPLUSEQUAL = 36\\nMINEQUAL = 37\\nSTAREQUAL = 38\\nSLASHEQUAL = 39\\nPERCENTEQUAL = 40\\nAMPEREQUAL = 41\\nVBAREQUAL = 42\\nCIRCUMFLEXEQUAL = 43\\nLEFTSHIFTEQUAL = 44\\nRIGHTSHIFTEQUAL = 45\\nDOUBLESTAREQUAL = 46\\nDOUBLESLASH = 47\\nDOUBLESLASHEQUAL = 48\\nAT = 49\\nATEQUAL = 50\\nRARROW = 51\\nELLIPSIS = 52\\nCOLONEQUAL = 53\\nOP = 54\\nAWAIT = 55\\nASYNC = 56\\nTYPE_IGNORE = 57\\nTYPE_COMMENT = 58\\n# These aren't used by the C tokenizer but are needed for tokenize.py\\nERRORTOKEN = 59\\nCOMMENT = 60\\nNL = 61\\nENCODING = 62\\nN_TOKENS = 63\\n# Special definitions for cooperation with parser\\nNT_OFFSET = 256\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioQrKfTs8Ti4"
      },
      "source": [
        "#https://docs.python.org/3/library/tokenize.html\r\n",
        "def tokenize_python(code_snippet):\r\n",
        "    tokens = tokenize(io.BytesIO(code_snippet.encode('utf-8')).readline)\r\n",
        "    parsed = []\r\n",
        "    for token in tokens:\r\n",
        "        if token.type not in [0,59,60,61,62,63,256]:\r\n",
        "            parsed.append(token.string)\r\n",
        "    return parsed"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8k7Lf6YDHo1",
        "outputId": "4b1054d0-cea4-45d7-cbee-b7efd4c8282f"
      },
      "source": [
        "tokenize_python(df['Code'][1])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['utf-8',\n",
              " '\\n',\n",
              " 'num1',\n",
              " '=',\n",
              " '1.5',\n",
              " '\\n',\n",
              " 'num2',\n",
              " '=',\n",
              " '6.3',\n",
              " '\\n',\n",
              " 'sum',\n",
              " '=',\n",
              " 'num1',\n",
              " '+',\n",
              " 'num2',\n",
              " '\\n',\n",
              " 'print',\n",
              " '(',\n",
              " \"f'Sum: {sum}'\",\n",
              " ')',\n",
              " '\\n',\n",
              " '\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oC-bUR5h9-Z"
      },
      "source": [
        "import spacy\r\n",
        "spacy_en = spacy.load('en')\r\n",
        "\r\n",
        "def tokenize_en(text):\r\n",
        "    \"\"\"\r\n",
        "    Tokenizes English text from a string into a list of strings\r\n",
        "    \"\"\"\r\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\r\n",
        "\r\n",
        "SRC = Field(tokenize= tokenize_en, \r\n",
        "            init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True,\r\n",
        "            batch_first=True)\r\n",
        "\r\n",
        "TRG = Field(tokenize = tokenize_python, \r\n",
        "            init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=False,\r\n",
        "            batch_first=True)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "schGM4YMkRXl"
      },
      "source": [
        "fields = [('Description', SRC),('Code',TRG)]\r\n"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K63Pacf9dP1C"
      },
      "source": [
        "# Using tabular dataset to process the text\r\n",
        "\r\n",
        "train_data, test_data = TabularDataset.splits(\r\n",
        "                                path = '',   \r\n",
        "                                train = './train.csv',\r\n",
        "                                test = './valid.csv',\r\n",
        "                                format = 'csv',\r\n",
        "                                fields = fields)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSyYnqeOhiZN"
      },
      "source": [
        "BATCH_SIZE = 24\r\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frL3JFUIkoeW"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 3,max_size= 10000)\r\n",
        "TRG.build_vocab(test_data, min_freq = 3,max_size= 10000)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reHXLuvMkvTV"
      },
      "source": [
        "train_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE,\r\n",
        "    sort_key = lambda x: len(x.Description),\r\n",
        "    device = device)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG_rKtAbmKPg"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 encoder, \r\n",
        "                 decoder, \r\n",
        "                 src_pad_idx, \r\n",
        "                 trg_pad_idx, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "    def make_src_mask(self, src):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        \r\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "\r\n",
        "        return src_mask\r\n",
        "    \r\n",
        "    def make_trg_mask(self, trg):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        \r\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        \r\n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\r\n",
        "        \r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\r\n",
        "        \r\n",
        "        #trg_sub_mask = [trg len, trg len]\r\n",
        "            \r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        \r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "                \r\n",
        "        src_mask = self.make_src_mask(src)\r\n",
        "        trg_mask = self.make_trg_mask(trg)\r\n",
        "        \r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "\r\n",
        "\r\n",
        "        enc_src = self.encoder(src, src_mask)\r\n",
        "        \r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "                \r\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]        \r\n",
        "        return output, attention"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fllX7D6mqDqM"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 input_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,\r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 2000):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim,\r\n",
        "                                                  dropout, \r\n",
        "                                                  device) \r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        batch_size = src.shape[0]\r\n",
        "        src_len = src.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [batch size, src len]\r\n",
        "\r\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\r\n",
        "\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            src = layer(src, src_mask)\r\n",
        "            \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        " \r\n",
        "            \r\n",
        "        return src"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wgnIzCiqEp2"
      },
      "source": [
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,  \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        #src_mask = [batch size, 1, 1, src len] \r\n",
        "                \r\n",
        "        #self attention\r\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _src = self.positionwise_feedforward(src)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        return src"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzFPJ5YpqNOT"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        assert hid_dim % n_heads == 0\r\n",
        "        \r\n",
        "        self.hid_dim = hid_dim\r\n",
        "        self.n_heads = n_heads\r\n",
        "        self.head_dim = hid_dim // n_heads\r\n",
        "        \r\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, query, key, value, mask = None):\r\n",
        "        \r\n",
        "        batch_size = query.shape[0]\r\n",
        "        \r\n",
        "        #query = [batch size, query len, hid dim]\r\n",
        "        #key = [batch size, key len, hid dim]\r\n",
        "        #value = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = self.fc_q(query)\r\n",
        "        K = self.fc_k(key)\r\n",
        "        V = self.fc_v(value)\r\n",
        "        \r\n",
        "        #Q = [batch size, query len, hid dim]\r\n",
        "        #K = [batch size, key len, hid dim]\r\n",
        "        #V = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        \r\n",
        "        #Q = [batch size, n heads, query len, head dim]\r\n",
        "        #K = [batch size, n heads, key len, head dim]\r\n",
        "        #V = [batch size, n heads, value len, head dim]\r\n",
        "                \r\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\r\n",
        "        \r\n",
        "        #energy = [batch size, n heads, query len, key len]\r\n",
        "        \r\n",
        "        if mask is not None:\r\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\r\n",
        "        \r\n",
        "        attention = torch.softmax(energy, dim = -1)\r\n",
        "                \r\n",
        "        #attention = [batch size, n heads, query len, key len]\r\n",
        "                \r\n",
        "        x = torch.matmul(self.dropout(attention), V)\r\n",
        "        \r\n",
        "        #x = [batch size, n heads, query len, head dim]\r\n",
        "        \r\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\r\n",
        "        \r\n",
        "        #x = [batch size, query len, n heads, head dim]\r\n",
        "        \r\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        x = self.fc_o(x)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        return x, attention"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqcTMtF8qQQl"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\r\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, pf dim]\r\n",
        "        \r\n",
        "        x = self.fc_2(x)\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOAuxKl2qVoy"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 2000):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim, \r\n",
        "                                                  dropout, \r\n",
        "                                                  device)\r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "                \r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "                            \r\n",
        "        #pos = [batch size, trg len]\r\n",
        "            \r\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\r\n",
        "                \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        output = self.fc_out(trg)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "            \r\n",
        "        return output, attention"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5BAIFqIqcGp"
      },
      "source": [
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        #self attention\r\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "            \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "            \r\n",
        "        #encoder attention\r\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\r\n",
        "        # query, key, value\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "                    \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _trg = self.positionwise_feedforward(trg)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        return trg, attention"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfSlm0Z0mUK1"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "HID_DIM = 256\r\n",
        "ENC_LAYERS = 2\r\n",
        "DEC_LAYERS = 2\r\n",
        "ENC_HEADS = 8\r\n",
        "DEC_HEADS = 8\r\n",
        "ENC_PF_DIM = 512\r\n",
        "DEC_PF_DIM = 512\r\n",
        "ENC_DROPOUT = 0.1\r\n",
        "DEC_DROPOUT = 0.1\r\n",
        "\r\n",
        "enc = Encoder(INPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              ENC_LAYERS, \r\n",
        "              ENC_HEADS, \r\n",
        "              ENC_PF_DIM, \r\n",
        "              ENC_DROPOUT, \r\n",
        "              device)\r\n",
        "\r\n",
        "\r\n",
        "dec = Decoder(OUTPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              DEC_LAYERS, \r\n",
        "              DEC_HEADS, \r\n",
        "              DEC_PF_DIM, \r\n",
        "              DEC_DROPOUT,\r\n",
        "              device)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2DLlhJSmXpU"
      },
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\r\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey774CaLmgsI",
        "outputId": "f0c5a18e-a1b8-4f84-d0d4-e5fe529f7d18"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,353,605 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2wspF4Cmnpz"
      },
      "source": [
        "def initialize_weights(m):\r\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\r\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfvJg4d9mr7c"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\r\n"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oKo8njwmt_g"
      },
      "source": [
        "model.apply(initialize_weights);\r\n"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i22PXjHmysa"
      },
      "source": [
        "\r\n",
        "LEARNING_RATE = 0.0005\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A-uyR-vm2y1"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch.Description\r\n",
        "        trg = batch.Code\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "\r\n",
        "                \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "\r\n",
        "            \r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "                \r\n",
        "        #output = [batch size * trg len - 1, output dim]\r\n",
        "        #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        #loss = maskNLLLoss(output, trg,model.trg_pad_idx)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRMf8Glpm5zj"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch.Description\r\n",
        "            trg = batch.Code\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "            \r\n",
        "            #output = [batch size, trg len - 1, output dim]\r\n",
        "            #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "            output_dim = output.shape[-1]\r\n",
        "           \r\n",
        "            \r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "            \r\n",
        "            #output = [batch size * trg len - 1, output dim]\r\n",
        "            #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "            \r\n",
        "            loss = criterion(output, trg)\r\n",
        "            #loss = maskNLLLoss(output, trg,model.trg_pad_idx)\r\n",
        "\r\n",
        "            #loss,_ = maskNLLLoss(output, trg, mask)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H25R2prm9Dc"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAWxVNo_m__A",
        "outputId": "3fdc45e1-fe0a-4d8b-d19d-60430f1c1445"
      },
      "source": [
        "import time\r\n",
        "N_EPOCHS = 10\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 5m 24s\n",
            "\tTrain Loss: 2.887 | Train PPL:  17.941\n",
            "\t Val. Loss: 2.306 |  Val. PPL:  10.037\n",
            "Epoch: 02 | Time: 5m 17s\n",
            "\tTrain Loss: 2.000 | Train PPL:   7.391\n",
            "\t Val. Loss: 1.956 |  Val. PPL:   7.073\n",
            "Epoch: 03 | Time: 5m 11s\n",
            "\tTrain Loss: 1.721 | Train PPL:   5.587\n",
            "\t Val. Loss: 1.756 |  Val. PPL:   5.787\n",
            "Epoch: 04 | Time: 5m 19s\n",
            "\tTrain Loss: 1.520 | Train PPL:   4.572\n",
            "\t Val. Loss: 1.598 |  Val. PPL:   4.942\n",
            "Epoch: 05 | Time: 5m 20s\n",
            "\tTrain Loss: 1.380 | Train PPL:   3.975\n",
            "\t Val. Loss: 1.504 |  Val. PPL:   4.502\n",
            "Epoch: 06 | Time: 5m 21s\n",
            "\tTrain Loss: 1.263 | Train PPL:   3.535\n",
            "\t Val. Loss: 1.404 |  Val. PPL:   4.071\n",
            "Epoch: 07 | Time: 5m 26s\n",
            "\tTrain Loss: 1.169 | Train PPL:   3.220\n",
            "\t Val. Loss: 1.344 |  Val. PPL:   3.834\n",
            "Epoch: 08 | Time: 5m 20s\n",
            "\tTrain Loss: 1.091 | Train PPL:   2.977\n",
            "\t Val. Loss: 1.283 |  Val. PPL:   3.606\n",
            "Epoch: 09 | Time: 5m 20s\n",
            "\tTrain Loss: 1.019 | Train PPL:   2.771\n",
            "\t Val. Loss: 1.258 |  Val. PPL:   3.518\n",
            "Epoch: 10 | Time: 5m 23s\n",
            "\tTrain Loss: 0.957 | Train PPL:   2.605\n",
            "\t Val. Loss: 1.205 |  Val. PPL:   3.337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTTR5lEOnDoq"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 100):\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "        \r\n",
        "    if isinstance(sentence, str):\r\n",
        "        nlp = spacy.load('en')\r\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in sentence]\r\n",
        "\r\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\r\n",
        "        \r\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n",
        "    src_mask = model.make_src_mask(src_tensor)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        enc_src = model.encoder(src_tensor,src_mask)\r\n",
        "\r\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n",
        "\r\n",
        "    for i in range(max_len):\r\n",
        "\r\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\r\n",
        "        with torch.no_grad():\r\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        pred_token = output.argmax(2)[:,-1].item()\r\n",
        "        \r\n",
        "        trg_indexes.append(pred_token)\r\n",
        "\r\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n",
        "            break\r\n",
        "    \r\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n",
        "    \r\n",
        "    return trg_tokens[1:]#, attention"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mysK3vRHKLQP",
        "outputId": "cf567314-2f3f-4608-8dab-dca6b5cb0c72"
      },
      "source": [
        "sentence = \"write a program to find and print the largest among three numbers\"\r\n",
        "code = translate_sentence(sentence, SRC, TRG, model, device)\r\n",
        "print(f'predicted trg = {code}')\r\n",
        "#print(f'predicted trg = {\" \".join(code)}')\r\n"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['utf-8', 'num1', '=', '10', '\\n', 'num2', '=', '12', '\\n', 'num3', '=', '12', '\\n', 'num3', '=', '12', '\\n', 'if', '(', 'num1', '>=', 'num3', ')', 'and', '(', 'num1', '>=', 'num3', ')', ':', '\\n', '   ', 'largest', '=', 'num2', '\\n', '', 'elif', '(', 'num2', '>=', 'num1', ')', 'and', '(', 'num2', '>=', 'num3', ')', ':', '\\n', '   ', 'largest', '=', 'num2', '\\n', '', 'elif', '(', 'num2', '>=', 'num3', ')', ':', '\\n', '   ', 'largest', '=', 'num3', '\\n', '', 'elif', '(', 'num2', '>=', 'num3', ')', ':', '\\n', '', 'elif', '(', 'num1', ')', ':', '\\n', '   ', 'largest', '=', 'num3', '\\n', '', 'elif', '(', 'num2', '\\n', '   ', 'largest', '=', 'num3']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i7MaYQIKqdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af63f0d-9026-4794-a3c3-c8c48bad586f"
      },
      "source": [
        "print(\"\".join(code))"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "utf-8num1=10\n",
            "num2=12\n",
            "num3=12\n",
            "num3=12\n",
            "if(num1>=num3)and(num1>=num3):\n",
            "   largest=num2\n",
            "elif(num2>=num1)and(num2>=num3):\n",
            "   largest=num2\n",
            "elif(num2>=num3):\n",
            "   largest=num3\n",
            "elif(num2>=num3):\n",
            "elif(num1):\n",
            "   largest=num3\n",
            "elif(num2\n",
            "   largest=num3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWjPAUJ6QA9l",
        "outputId": "0aaf8210-0196-4f41-bf81-d3959169af4b"
      },
      "source": [
        "sentence = \"write a program to add two numbers\"\r\n",
        "code = translate_sentence(sentence, SRC, TRG, model, device)\r\n",
        "print(f'predicted trg = {code}')\r\n",
        "print(f'predicted trg = {\" \".join(code)}')\r\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['utf-8', 'num1', '=', '<unk>', '\\n', 'num2', '=', '<unk>', '\\n', 'sum', '=', 'num1', '+', 'num2', '\\n', 'print', '(', '<unk>', ')', '', '<eos>']\n",
            "predicted trg = utf-8 num1 = <unk> \n",
            " num2 = <unk> \n",
            " sum = num1 + num2 \n",
            " print ( <unk> )  <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di3dwK8RQS0Q",
        "outputId": "b11b34b6-8c6f-4fa7-f32b-3e22c4bf061d"
      },
      "source": [
        "sentence = \"write a program to multiply two numbers\"\r\n",
        "code = translate_sentence(sentence, SRC, TRG, model, device)\r\n",
        "print(f'predicted trg = {code}\\n')\r\n",
        "print(\" \".join(code))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['utf-8', 'num1', '=', '<unk>', '\\n', 'num2', '=', '<unk>', '\\n', 'num2', '=', 'num1', '*', 'num2', '\\n', 'print', '(', '<unk>', ')', '', '<eos>']\n",
            "\n",
            "utf-8 num1 = <unk> \n",
            " num2 = <unk> \n",
            " num2 = num1 * num2 \n",
            " print ( <unk> )  <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "638D43sYQkUO",
        "outputId": "39aecb53-4563-405b-aaad-705091424aee"
      },
      "source": [
        "sentence = \"write a program to print factorial of a number\"\r\n",
        "code = translate_sentence(sentence, SRC, TRG, model, device)\r\n",
        "print(f'predicted trg = {code}\\n')\r\n",
        "print(\" \".join(code))"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['utf-8', 'num', '=', 'int', '(', 'input', '(', '\"Enter a number: \"', ')', ')', '\\n', 'factorial', '=', '1', '\\n', 'if', 'num', '==', '0', ':', '\\n', '   ', 'print', '(', '\"Sorry, factorial does not exist for negative numbers\"', ')', '\\n', '', 'else', ':', '\\n', '   ', 'print', '(', '\"The factorial of 0 is 1\"', ')', '\\n', '', '', '<eos>']\n",
            "\n",
            "utf-8 num = int ( input ( \"Enter a number: \" ) ) \n",
            " factorial = 1 \n",
            " if num == 0 : \n",
            "     print ( \"Sorry, factorial does not exist for negative numbers\" ) \n",
            "  else : \n",
            "     print ( \"The factorial of 0 is 1\" ) \n",
            "   <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfyKB9JDQw0y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}