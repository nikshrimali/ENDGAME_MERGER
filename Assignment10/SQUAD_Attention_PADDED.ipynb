{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQUAD-Attention_PADDED.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeXyQZV/XUTBxr5gKYHxBf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikshrimali/ENDGAME_MERGER/blob/main/Assignment10/SQUAD_Attention_PADDED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFkTxnkRfZLj"
      },
      "source": [
        "## Importing Libraries\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzVEXREHewvV",
        "outputId": "822f915c-8556-45b4-ae20-e5d975998188"
      },
      "source": [
        "# Importing torch and essential libraries\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(f'Currently running on {device}')\r\n",
        "\r\n",
        "import spacy\r\n",
        "spacy_en = spacy.load('en')\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import random\r\n",
        "import math\r\n",
        "import time\r\n",
        "import json\r\n",
        "import random"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently running on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7LjRu4ScquO",
        "outputId": "0b3663a1-6a58-4ac3-b7f1-b9979cf630cb"
      },
      "source": [
        "# Getting the dataset\r\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\r\n",
        "\r\n",
        "# Getting the test dataset\r\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-13 02:09:52--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.108.153, 185.199.111.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M   225MB/s    in 0.2s    \n",
            "\n",
            "2021-01-13 02:09:52 (225 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n",
            "--2021-01-13 02:09:52--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.108.153, 185.199.111.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json’\n",
            "\n",
            "dev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-01-13 02:09:52 (73.1 MB/s) - ‘dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrA79pVgfHPs"
      },
      "source": [
        "# Setting seeds for reproducability\r\n",
        "\r\n",
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE-xCNyefTOr"
      },
      "source": [
        "## Loading Json and formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR6JT30ZfN5z"
      },
      "source": [
        "with open(\"train-v2.0.json\") as f:\r\n",
        "    train_dict = json.load(f)\r\n",
        "\r\n",
        "with open(\"/content/dev-v2.0.json\") as f:\r\n",
        "    test_dict = json.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyG6aGaZfkT1"
      },
      "source": [
        "def json_to_df(json_dict):\r\n",
        "    '''\r\n",
        "    Takes input as a dictionary and returns a dataframe of columns InputData and Answer\r\n",
        "    Currently returns the dataframe upto 10k rows due to storage constraints\r\n",
        "    '''\r\n",
        "    df = pd.DataFrame(columns=['InputData', 'Answer'])\r\n",
        "    df_idx = 0\r\n",
        "    for topic in json_dict[\"data\"]:\r\n",
        "        for pgraph in topic[\"paragraphs\"]:\r\n",
        "            \r\n",
        "            for index, qa in enumerate(pgraph[\"qas\"]):\r\n",
        "                if not qa[\"is_impossible\"]:\r\n",
        "                    text = pgraph[\"context\"]\r\n",
        "                    question = qa[\"question\"]\r\n",
        "                    df.at[df_idx, 'InputData'] = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\r\n",
        "                    df.at[df_idx, 'Answer'] = qa[\"answers\"][0]['text']\r\n",
        "                    df_idx += 1\r\n",
        "                \r\n",
        "    return df[:10000]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP8sf2BbfqP8"
      },
      "source": [
        "def get_pandas_data():\r\n",
        "\r\n",
        "    '''Reads the pandas data if already exists'''\r\n",
        "\r\n",
        "    if not os.path.exists('/content/train_data.csv'):\r\n",
        "        train_data = json_to_df(train_dict)\r\n",
        "        test_data = json_to_df(test_dict)\r\n",
        "        train_data.to_csv('train_data.csv', index=False)\r\n",
        "        test_data.to_csv('test_data.csv', index=False)\r\n",
        "    else:\r\n",
        "        train_data = pd.read_csv('/content/train_data.csv')\r\n",
        "        test_data = pd.read_csv('/content/test_data.csv')\r\n",
        "    return train_data, test_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDmnVhMeg_xE"
      },
      "source": [
        "train_data, test_data = get_pandas_data()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "C1h3lwRehB-9",
        "outputId": "6cfff690-1f1e-45e3-ae8f-c683cca641ff"
      },
      "source": [
        "# Lets see what our output looks like\r\n",
        "print(train_data.head(10))\r\n",
        "print(test_data.head(10))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a39ddbd2c9b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lets see what our output looks like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxuJPZNQhjtn"
      },
      "source": [
        "## Converting the dataset into processable format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdh_C66PhVi1"
      },
      "source": [
        "def tokenize_en(text):\r\n",
        "    \"\"\"\r\n",
        "    Tokenizes English text from a string into a list of strings\r\n",
        "    \"\"\"\r\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d43fJtUiFAq"
      },
      "source": [
        "### Field is like a tuple that converts the data into SRC and TRG format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9-5ym-UiBXI"
      },
      "source": [
        "SRC = Field(tokenize= tokenize_en, \r\n",
        "            init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True)\r\n",
        "\r\n",
        "TRG = Field(tokenize = tokenize_en, \r\n",
        "            init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True)\r\n",
        "\r\n",
        "fields = {'InputData': ('q', SRC), 'Answer': ('t', TRG)}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pggppR_KijbU"
      },
      "source": [
        "train_data, test_data = TabularDataset.splits(\r\n",
        "                                path = '',   \r\n",
        "                                train = 'train_data.csv',\r\n",
        "                                test = 'test_data.csv',\r\n",
        "                                format = 'csv',\r\n",
        "                                fields = fields)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwmx3Wx4inWd"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2, max_size= 10000)\r\n",
        "TRG.build_vocab(train_data, min_freq = 2, max_size= 5000)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uJ0nxd1iyAL"
      },
      "source": [
        "BATCH_SIZE = 24\r\n",
        "\r\n",
        "train_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE,\r\n",
        "    sort=False,\r\n",
        "    device = device)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe01cBQxi87D"
      },
      "source": [
        "# Modellling\r\n",
        "\r\n",
        "## Attention Mechanism \r\n",
        "\r\n",
        "It will primarily have 3 components, first is Encoder, decoder, attention block and then connecting all this into a sequence is a SEQ-to-SEQ block of code\r\n",
        "\r\n",
        "## Encoder\r\n",
        "\r\n",
        "This block takes inputs of a particular size which is of dimenstion of vocab, takes hidden dimension, embedding dimension as we are training the embedding as well. It is a bi-directional GRU block, hence it outputs would be an hidden state and outputs\r\n",
        "\r\n",
        "## Decoder \r\n",
        "\r\n",
        "This block takes inputs from the encoder and the attention block. This is also a bi-directional GRU block which will have a linear layer attached along with it.\r\n",
        "\r\n",
        "## Attention block\r\n",
        "\r\n",
        "This block takes hidden state of encoder and also takes the hidden state of the decoder, and generates a similarity score between them, which helps decoder to focus on a particular section of the code rather than all of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl-XzcYlk0Va"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_dim, embedding_dim, enc_hidden_dim, dec_hid_dim dropout_size):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # Embedding Hyperparameters\r\n",
        "        # num_embeddings = Size of your input of your vocab\r\n",
        "        # embedding dim = Size of your embeddings dimension\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\r\n",
        "        # From here we send our embeddings to a RNN which then generates output\r\n",
        "\r\n",
        "        # Dropout is used in some layers of embeddings and hidden states\r\n",
        "        self.dropout = nn.Dropout(self.dropout_size)\r\n",
        "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=enc_hidden_dim, bidirectional=True)\r\n",
        "        self.fc = nn.Linear(enc_hid_dim*2, dec_hid_dim) # enc_hid_dim * 2 because the nn is bidirectional in nature\r\n",
        "\r\n",
        "    def forward(self, input_src):\r\n",
        "        # src = [src_len, batch_size]\r\n",
        "        embedded_data = self.dropout(self.embedding(input_src))\r\n",
        "        # embedded = [src_len, batch_size, embedding_dim]\r\n",
        "        output, hidden = self.rnn(embedded_data)\r\n",
        "\r\n",
        "        # output = [src_len, batch_size, hidden_dim*num_directions]\r\n",
        "        # hidden = [n_layers*num_direction, batch_size, hidden_dim]\r\n",
        "        \r\n",
        "\r\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:], dim=1))))\r\n",
        "        # hidden_dim = [batch_size, dec hid dim]\r\n",
        "        return outputs, hidden\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AYmer0vqv0s"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, output_dim, embedding_dim,enc_hidden_dim, dec_hidden_dim, dropout_size, attention):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(num_embeddings=output_dim, embedding_dim=embedding_dim)\r\n",
        "        # Output shape of embedding_dim is input_size, embedding_dim\r\n",
        "        self.dropout = nn.Dropout(dropout_size)\r\n",
        "        self.rnn = nn.GRU(input_size=embedding_dim, bidirectional=False, n_layers=1, hidden_size=dec_hidden_dim)\r\n",
        "        # Output shape of GRU is hidden is of shape - batch_size, input_dim, hidden_dim when unidirectional\r\n",
        "        # Linear layers takes input from embedding layers, attention block, \r\n",
        "        self.linear = nn.Linear(in_features=(embedding_dim + enc_hidden_dim*2 + dec_hidden_dim), output_dim)\r\n",
        "        # Output shape of linear  layers is concat of all(input, hidden, embedding)\r\n",
        "\r\n",
        "    def forward(self, output_trg):\r\n",
        "\r\n",
        "        embedded_data = self.dropout(self.embedding(output_trg))\r\n",
        "        output, hidden = self.rnn(embedded_data)\r\n",
        "\r\n",
        "        # Hidden is stacked forwards and backwards\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjIoOlwBwuNL"
      },
      "source": [
        "class Attention(nn.Module):\r\n",
        "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # Attention is basically a dot product between the outputs from the encoder and output from the decoder\r\n",
        "        self.attn = nn.Linear((enc_hidden_dim*2) + dec_hidden_dim, dec_hidden_dim)\r\n",
        "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\r\n",
        "\r\n",
        "    def forward(self, hidden, encoder_outputs):\r\n",
        "        # hidden = [batch_size, dec hid dim]\r\n",
        "        # encoder_outputs = [src_len, batch_size, enc_hid_dim*2]\r\n",
        "        \r\n",
        "        batch_size = encoder_outputs.shape[1]\r\n",
        "        src_len = encoder_outputs.shape[0]\r\n",
        "\r\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\r\n",
        "        encoder_outputs = encoder_outputs.permute(1,0,2)\r\n",
        "        energy = torch.tanh(self.attn(torch.cat(hidden, encoder_outputs), dim=2))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf9gbbTNp9Nc"
      },
      "source": [
        ""
      ]
    }
  ]
}