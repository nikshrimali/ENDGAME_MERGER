{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SQUAD-Attention_PADDED.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCXNFFR3hIg4FpI+9Ku78p",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikshrimali/ENDGAME_MERGER/blob/main/Assignment10/SQUAD_Attention_PADDED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFkTxnkRfZLj"
      },
      "source": [
        "## Importing Libraries\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzVEXREHewvV",
        "outputId": "a68c024d-16fe-4dbb-ee16-858ed51812ad"
      },
      "source": [
        "# Importing torch and essential libraries\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(f'Currently running on {device}')\r\n",
        "\r\n",
        "import spacy\r\n",
        "spacy_en = spacy.load('en')\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import random\r\n",
        "import math\r\n",
        "import time\r\n",
        "import json\r\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently running on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7LjRu4ScquO",
        "outputId": "5b19fd52-ff1f-42e4-f96f-0cba107c92b2"
      },
      "source": [
        "# Getting the dataset\r\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\r\n",
        "\r\n",
        "# Getting the test dataset\r\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-17 16:35:59--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M  91.6MB/s    in 0.4s    \n",
            "\n",
            "2021-01-17 16:36:00 (91.6 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n",
            "--2021-01-17 16:36:00--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json’\n",
            "\n",
            "dev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-01-17 16:36:00 (61.9 MB/s) - ‘dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrA79pVgfHPs"
      },
      "source": [
        "# Setting seeds for reproducability\r\n",
        "\r\n",
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE-xCNyefTOr"
      },
      "source": [
        "## Loading Json and formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR6JT30ZfN5z"
      },
      "source": [
        "with open(\"train-v2.0.json\") as f:\r\n",
        "    train_dict = json.load(f)\r\n",
        "\r\n",
        "with open(\"/content/dev-v2.0.json\") as f:\r\n",
        "    test_dict = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyG6aGaZfkT1"
      },
      "source": [
        "def json_to_df(json_dict):\r\n",
        "    '''\r\n",
        "    Takes input as a dictionary and returns a dataframe of columns InputData and Answer\r\n",
        "    Currently returns the dataframe upto 10k rows due to storage constraints\r\n",
        "    '''\r\n",
        "    df = pd.DataFrame(columns=['InputData', 'Answer'])\r\n",
        "    df_idx = 0\r\n",
        "    for topic in json_dict[\"data\"]:\r\n",
        "        for pgraph in topic[\"paragraphs\"]:\r\n",
        "            \r\n",
        "            for index, qa in enumerate(pgraph[\"qas\"]):\r\n",
        "                if not qa[\"is_impossible\"]:\r\n",
        "                    text = pgraph[\"context\"]\r\n",
        "                    question = qa[\"question\"]\r\n",
        "                    df.at[df_idx, 'InputData'] = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\r\n",
        "                    df.at[df_idx, 'Answer'] = qa[\"answers\"][0]['text']\r\n",
        "                    df_idx += 1\r\n",
        "                \r\n",
        "    return df[:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP8sf2BbfqP8"
      },
      "source": [
        "def get_pandas_data():\r\n",
        "\r\n",
        "    '''Reads the pandas data if already exists'''\r\n",
        "\r\n",
        "    if not os.path.exists('/content/train_data.csv'):\r\n",
        "        train_data = json_to_df(train_dict)\r\n",
        "        test_data = json_to_df(test_dict)\r\n",
        "        train_data.to_csv('train_data.csv', index=False)\r\n",
        "        test_data.to_csv('test_data.csv', index=False)\r\n",
        "    else:\r\n",
        "        train_data = pd.read_csv('/content/train_data.csv')\r\n",
        "        test_data = pd.read_csv('/content/test_data.csv')\r\n",
        "    return train_data, test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDmnVhMeg_xE"
      },
      "source": [
        "train_data, test_data = get_pandas_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1h3lwRehB-9",
        "outputId": "243a2936-9c03-4f2d-b96a-7954f1b959a3"
      },
      "source": [
        "# Lets see what our output looks like\r\n",
        "print(train_data.head(10))\r\n",
        "print(test_data.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                           InputData               Answer\n",
            "0  [CLS] When did Beyonce start becoming popular?...    in the late 1990s\n",
            "1  [CLS] What areas did Beyonce compete in when s...  singing and dancing\n",
            "2  [CLS] When did Beyonce leave Destiny's Child a...                 2003\n",
            "3  [CLS] In what city and state did Beyonce  grow...       Houston, Texas\n",
            "4  [CLS] In which decade did Beyonce become famou...           late 1990s\n",
            "5  [CLS] In what R&B group was she the lead singe...      Destiny's Child\n",
            "6  [CLS] What album made her a worldwide known ar...  Dangerously in Love\n",
            "7  [CLS] Who managed the Destiny's Child group? [...       Mathew Knowles\n",
            "8  [CLS] When did Beyoncé rise to fame? [SEP] Bey...           late 1990s\n",
            "9  [CLS] What role did Beyoncé have in Destiny's ...          lead singer\n",
            "                                           InputData                       Answer\n",
            "0  [CLS] In what country is Normandy located? [SE...                       France\n",
            "1  [CLS] When were the Normans in Normandy? [SEP]...      10th and 11th centuries\n",
            "2  [CLS] From which countries did the Norse origi...  Denmark, Iceland and Norway\n",
            "3  [CLS] Who was the Norse leader? [SEP] The Norm...                        Rollo\n",
            "4  [CLS] What century did the Normans first gain ...                 10th century\n",
            "5  [CLS] Who was the duke in the battle of Hastin...        William the Conqueror\n",
            "6  [CLS] Who ruled the duchy of Normandy [SEP] Th...                    Richard I\n",
            "7  [CLS] What religion were the Normans [SEP] The...                     Catholic\n",
            "8  [CLS] What is the original meaning of the word...                       Viking\n",
            "9  [CLS] When was the Latin version of the word N...                  9th century\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxuJPZNQhjtn"
      },
      "source": [
        "## Converting the dataset into processable format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdh_C66PhVi1"
      },
      "source": [
        "def tokenize_en(text):\r\n",
        "    \"\"\"\r\n",
        "    Tokenizes English text from a string into a list of strings\r\n",
        "    \"\"\"\r\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d43fJtUiFAq"
      },
      "source": [
        "### Field is like a tuple that converts the data into SRC and TRG format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9-5ym-UiBXI"
      },
      "source": [
        "SRC = Field(tokenize= tokenize_en, \r\n",
        "            init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True)\r\n",
        "\r\n",
        "TRG = Field(tokenize = tokenize_en, \r\n",
        "            init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True)\r\n",
        "\r\n",
        "fields = {'InputData': ('q', SRC), 'Answer': ('t', TRG)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pggppR_KijbU"
      },
      "source": [
        "train_data, test_data = TabularDataset.splits(\r\n",
        "                                path = '',   \r\n",
        "                                train = 'train_data.csv',\r\n",
        "                                test = 'test_data.csv',\r\n",
        "                                format = 'csv',\r\n",
        "                                fields = fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwmx3Wx4inWd"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2, max_size= 10000)\r\n",
        "TRG.build_vocab(train_data, min_freq = 2, max_size= 5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uJ0nxd1iyAL"
      },
      "source": [
        "BATCH_SIZE = 24\r\n",
        "\r\n",
        "train_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE,\r\n",
        "    sort=False,\r\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe01cBQxi87D"
      },
      "source": [
        "# Modellling\r\n",
        "\r\n",
        "## Attention Mechanism \r\n",
        "\r\n",
        "It will primarily have 3 components, first is Encoder, decoder, attention block and then connecting all this into a sequence is a SEQ-to-SEQ block of code\r\n",
        "\r\n",
        "## Encoder\r\n",
        "\r\n",
        "This block takes inputs of a particular size which is of dimenstion of vocab, takes hidden dimension, embedding dimension as we are training the embedding as well. It is a bi-directional GRU block, hence it outputs would be an hidden state and outputs\r\n",
        "\r\n",
        "## Decoder \r\n",
        "\r\n",
        "This block takes inputs from the encoder and the attention block. This is also a bi-directional GRU block which will have a linear layer attached along with it.\r\n",
        "\r\n",
        "## Attention block\r\n",
        "\r\n",
        "This block takes hidden state of encoder and also takes the hidden state of the decoder, and generates a similarity score between them, which helps decoder to focus on a particular section of the code rather than all of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl-XzcYlk0Va"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_dim, embedding_dim, enc_hidden_dim, dec_hidden_dim, dropout_size):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # Embedding Hyperparameters\r\n",
        "        # num_embeddings = Size of your input of your vocab\r\n",
        "        # embedding dim = Size of your embeddings dimension\r\n",
        "        \r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\r\n",
        "        # From here we send our embeddings to a RNN which then generates output\r\n",
        "\r\n",
        "        # Dropout is used in layers of embeddings and hidden states\r\n",
        "        self.dropout = nn.Dropout(dropout_size)\r\n",
        "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=enc_hidden_dim, bidirectional=True)\r\n",
        "        self.fc = nn.Linear(enc_hidden_dim*2, dec_hidden_dim) # enc_hid_dim * 2 because the nn is bidirectional in nature\r\n",
        "        #Why fc output of fc layer is equal to the dec_hid_dim\r\n",
        "\r\n",
        "    def forward(self, input_src):\r\n",
        "        # src = [src_len, batch_size]\r\n",
        "        print(f'Shape of the input dim is [src_len, batch_size] -  {input_src.shape}')\r\n",
        "        embedded_data = self.dropout(self.embedding(input_src))\r\n",
        "        print(f'Shape of the embedding dim is [src_len, batch_size, embedded_dim] -  {embedded_data.shape}')\r\n",
        "        # embedded = [src_len, batch_size, embedding_dim]\r\n",
        "\r\n",
        "        src_len = input_src.shape[0].cpu()\r\n",
        "        # print(f'src_len = {src_len}')\r\n",
        "        packed_embedding = nn.utils.rnn.pack_padded_sequence(embedded_data, src_len)\r\n",
        "        packed_outputs, hidden = self.rnn(packed_embedding)\r\n",
        "\r\n",
        "        # Add padding back to the output to add gpu processing\r\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\r\n",
        "\r\n",
        "        print(f'Shape of the output from RNN is [src_len, batch_size, hidden_dim*num_directions] -  {output.shape}')\r\n",
        "        print(f'Shape of the hidden from RNN is [src_len, batch_size, hidden_dim*num_directions] -  {hidden.shape}')\r\n",
        "\r\n",
        "        # output = [src_len, batch_size, hidden_dim*num_directions]\r\n",
        "        # hidden = [n_layers*num_direction, batch_size, hidden_dim]\r\n",
        "        \r\n",
        "\r\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\r\n",
        "        \r\n",
        "        # hidden_dim = [batch_size, dec hid dim]\r\n",
        "        return output, hidden\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CynEFdEe54G5"
      },
      "source": [
        "# Attention mechanism\r\n",
        "\r\n",
        "class Attention(nn.Module):\r\n",
        "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # Attention is basically a dot product between the outputs from the encoder and output from the decoder\r\n",
        "        self.attn = nn.Linear((enc_hidden_dim*2) + dec_hidden_dim, dec_hidden_dim)\r\n",
        "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\r\n",
        "\r\n",
        "    def forward(self, hidden, encoder_outputs, mask):\r\n",
        "        # hidden = [batch_size, dec hid dim]\r\n",
        "        # encoder_outputs = [src_len, batch_size, enc_hid_dim*2]\r\n",
        "        \r\n",
        "        print(f'shape of encoder outputs {encoder_outputs.shape}')\r\n",
        "        batch_size = encoder_outputs.shape[1]\r\n",
        "        src_len = encoder_outputs.shape[0]\r\n",
        "\r\n",
        "        # Repeat decoder hidden state src_len times\r\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\r\n",
        "        print(f'shape of hidden after unsqueezing source lenght times {hidden.shape}')\r\n",
        "\r\n",
        "        encoder_outputs = encoder_outputs.permute(1,0,2)\r\n",
        "\r\n",
        "        # energy = [batch size, src len, enc_hidden_dim*2]\r\n",
        "        # energy is concat of encoder output \r\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\r\n",
        "\r\n",
        "        print(f'Shape of energy {energy.shape}')\r\n",
        "\r\n",
        "        attention = self.v(energy).squeeze(2)\r\n",
        "\r\n",
        "        # Fill the attention values of masked fill to very small values\r\n",
        "\r\n",
        "        attention = attention.masked_fill(mask==0, -1e10)\r\n",
        "\r\n",
        "        print(f'Shape of attention :', attention.shape)\r\n",
        "        # attention = [batch size, src len]\r\n",
        "\r\n",
        "        return F.softmax(attention, dim=1) # Get softmax outputs on dim = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AYmer0vqv0s"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, output_dim, embedding_dim,enc_hidden_dim, dec_hidden_dim, dropout_size, attention):\r\n",
        "        super().__init__()\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.attention = attention\r\n",
        "        \r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(num_embeddings=output_dim, embedding_dim=embedding_dim)\r\n",
        "\r\n",
        "        # Output shape of embedding_dim is input_size, embedding_dim\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout_size)\r\n",
        "\r\n",
        "        # Why enc_hidden_dim*2 - Input of attention is going into decoder rnn's along with embedding dim, and previous hidden state\r\n",
        "        self.rnn = nn.GRU(input_size=(enc_hidden_dim*2 + embedding_dim), bidirectional=False, hidden_size=dec_hidden_dim)\r\n",
        "\r\n",
        "        # Output shape of GRU is hidden is of shape - batch_size, input_dim, hidden_dim when unidirectional\r\n",
        "        # Linear layers takes input from embedding layers, attention block, \r\n",
        "\r\n",
        "        self.fc_out = nn.Linear(in_features=(embedding_dim + enc_hidden_dim*2 + dec_hidden_dim), out_features= output_dim)\r\n",
        "\r\n",
        "        # Output shape of linear  layers is concat of all(input, hidden, embedding)\r\n",
        "\r\n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\r\n",
        "\r\n",
        "        # input = [src_len, batche_size]\r\n",
        "        # hidden = [batch_size, dec_hidden_dim]\r\n",
        "\r\n",
        "        print(f'Input before unsqueeeze {input}, after unsqueeze {input.unsqueeze(0)}')\r\n",
        "\r\n",
        "        input = input.unsqueeze(0)\r\n",
        "\r\n",
        "        embedded_data = self.dropout(self.embedding(input))\r\n",
        "\r\n",
        "\r\n",
        "        a = self.attention(hidden, encoder_outputs, mask)\r\n",
        "\r\n",
        "        print('attention shape before unsqueeze', a.shape)\r\n",
        "        \r\n",
        "        a = a.unsqueeze(1)\r\n",
        "\r\n",
        "        print('attention shape after unsqueeze', a.shape)\r\n",
        "\r\n",
        "        output, hidden = self.rnn(embedded_data)\r\n",
        "\r\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n",
        "        # encoder_outputs = [src_len, batch_size, enc_hid_dim*2]\r\n",
        "\r\n",
        "        weighted = torch.bmm(a, encoder_outputs)\r\n",
        "\r\n",
        "        # print('Shape of weighted', weighted.shape)\r\n",
        "        # print('Shape of weighted', weighted.shape)\r\n",
        "        # print('Shape of weighted', weighted.shape)\r\n",
        "\r\n",
        "        print(f'Hidden shape before unsqueeze {hidden.shape} and after unsqueeze {hidden.unsqueeze(0).shape}')\r\n",
        "        rnn_input = self.rnn(embedded, weighted.unsqueeze(0))\r\n",
        "\r\n",
        "        # rnn_input = [1, batch_size, (enc_hidden_dim*2)+ emb_dim]\r\n",
        "\r\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\r\n",
        "\r\n",
        "        # output = [seq len, batch_size, dec_hid_dim*n_directions]\r\n",
        "\r\n",
        "        # hidden = [n_layers*n directions, batch_size, dec_hid_dim]\r\n",
        "\r\n",
        "        # seq len, n layers and n_directions are alwys 1 in this decoder-\r\n",
        "        # as the outputs are 1\r\n",
        "\r\n",
        "        embedded = embedded.squeeze(0)\r\n",
        "        output = output.squeeze(0)\r\n",
        "        weighted = weighted.squeeze(0)\r\n",
        "\r\n",
        "        print(f'embedded shape {embedded.shape}, output - {output.shape}, weighted = {weighted.shape}')\r\n",
        "\r\n",
        "         \r\n",
        "        # Hidden is stacked forwards and backwards\r\n",
        "        return prediction, hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjIoOlwBwuNL"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder, device, src_pad_idx):\r\n",
        "        super().__init__()\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.device = device\r\n",
        "        self. src_pad_idx =  src_pad_idx\r\n",
        "    \r\n",
        "    def crease_mask(self, src):\r\n",
        "        mask = (src != self.src_pad_idx).permute(1,0)\r\n",
        "        return mask\r\n",
        "\r\n",
        "    def forward(self, src, trg, teacher_forcing=0.5):\r\n",
        "        # src = [src len, batch_size]\r\n",
        "        # trg = [trg_len, batch_size]\r\n",
        "\r\n",
        "        batch_size = src.shape[1]\r\n",
        "        trg_len = trg.shape[0]\r\n",
        "\r\n",
        "        # Decoder output dim what is?\r\n",
        "        trg_vocab_size = self.decoder.output_dim\r\n",
        "\r\n",
        "        # tensor to store decoder outputs\r\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\r\n",
        "\r\n",
        "        # encoder outputs is all hidden states of the input sequence\r\n",
        "\r\n",
        "        encoder_outputs, hidden = self.encoder(src)\r\n",
        "\r\n",
        "        # first input is <sos> token\r\n",
        "        input = trg[0,:]\r\n",
        "        for t in range(1,trg_len):\r\n",
        "            mask = self.create_mask(src)\r\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs, mask)\r\n",
        "\r\n",
        "            outputs[t] = output\r\n",
        "\r\n",
        "            teacher_force = random.random() < teacher_force_ratio\r\n",
        "\r\n",
        "            # Highest predicted token from predictions\r\n",
        "            top1 = output.argmax(1)\r\n",
        "\r\n",
        "            # if teacher forcing, use actual next token as input\r\n",
        "            input = trg[t] if teacher_force else top1\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf9gbbTNp9Nc"
      },
      "source": [
        "## Training the Seq2Seq model\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkVPnI5fs5IE"
      },
      "source": [
        "\r\n",
        "INPUT_DIM = len(SRC.vocab)\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "ENC_EMB_DIM = 256\r\n",
        "DEC_EMB_DIM = 256\r\n",
        "ENC_HID_DIM = 512\r\n",
        "DEC_HID_DIM = 512\r\n",
        "ENC_DROPOUT = 0.5\r\n",
        "DEC_DROPOUT = 0.5\r\n",
        "\r\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\r\n",
        "\r\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\r\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\r\n",
        "            # input_dim, embedding_dim, enc_hidden_dim, dec_hidden_dim, dropout_size)\r\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\r\n",
        "                # output_dim, embedding_dim,enc_hidden_dim, dec_hidden_dim, dropout_size, attention\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, device, SRC_PAD_IDX).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_CoOeJxumZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953ac4b7-d373-4616-e90c-27bb9b5d370b"
      },
      "source": [
        "# weights and biases initialized to 0\r\n",
        "\r\n",
        "def init_weights(m):\r\n",
        "    for name, param in m.named_parameters():\r\n",
        "        if 'weight' in name:\r\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\r\n",
        "        else:\r\n",
        "            nn.init.constant_(param.data,0)\r\n",
        "\r\n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(2918, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(502, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=502, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2aZF-_ivZZa"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0h6BEkzvayJ"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJwV_nvHveIT"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch.q\r\n",
        "        trg = batch.t\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output = model(src, trg)\r\n",
        "        \r\n",
        "        #trg = [trg len, batch size]\r\n",
        "        #output = [trg len, batch size, output dim]\r\n",
        "        \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "        \r\n",
        "        output = output[1:].view(-1, output_dim)\r\n",
        "        trg = trg[1:].view(-1)\r\n",
        "        \r\n",
        "        #trg = [(trg len - 1) * batch size]\r\n",
        "        #output = [(trg len - 1) * batch size, output dim]\r\n",
        "        \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfBPoKZlvx7d"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch.q\r\n",
        "            trg = batch.t\r\n",
        "\r\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\r\n",
        "\r\n",
        "            #trg = [trg len, batch size]\r\n",
        "            #output = [trg len, batch size, output dim]\r\n",
        "\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output[1:].view(-1, output_dim)\r\n",
        "            trg = trg[1:].view(-1)\r\n",
        "\r\n",
        "            #trg = [(trg len - 1) * batch size]\r\n",
        "            #output = [(trg len - 1) * batch size, output dim]\r\n",
        "\r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfjmGhDGvzdK"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81CNH4yhv38P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "eaf01cb2-2a6c-4907-962f-b3b57920b441"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    print('Training started')\r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    print('training complete')\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "\r\n",
        "    end_time = time.time()\r\n",
        "    print(end_time)\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training started\n",
            "Shape of the input dim is [src_len, batch_size] -  torch.Size([433, 24])\n",
            "Shape of the embedding dim is [src_len, batch_size, embedded_dim] -  torch.Size([433, 24, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-91727bdaff28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training started'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-115-b45048135fa0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#trg = [trg len, batch size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-e46600d9d77c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# encoder outputs is all hidden states of the input sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# first input is <sos> token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-107-043eceff9922>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_src)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# embedded = [src_len, batch_size, embedding_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msrc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# print(f'src_len = {src_len}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mpacked_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'cpu'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJJQG11Iv7rE"
      },
      "source": [
        "\r\n",
        "model.load_state_dict(torch.load('tut3-model.pt'))\r\n",
        "\r\n",
        "test_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkUeexH6_Jfl"
      },
      "source": [
        "## Inferencing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5EwEybp_Qrd"
      },
      "source": [
        "example_idx = 12\r\n",
        "\r\n",
        "src = vars(train_data.examples[example_idx])['src']\r\n",
        "src = vars(train_data.examples[example_idx])['trg']\r\n",
        "\r\n",
        "print(f'src = {src}')\r\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZDI88KyAj9p"
      },
      "source": [
        "answer, attention = "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}