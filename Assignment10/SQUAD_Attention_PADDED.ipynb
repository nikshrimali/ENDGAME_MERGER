{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SQUAD-Attention_PADDED.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6SLqz2JQnc8B2PIswdshS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikshrimali/ENDGAME_MERGER/blob/main/Assignment10/SQUAD_Attention_PADDED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFkTxnkRfZLj"
      },
      "source": [
        "## Importing Libraries\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzVEXREHewvV",
        "outputId": "c1317aa8-fa35-4934-a8b3-3e8598d10078"
      },
      "source": [
        "# Importing torch and essential libraries\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(f'Currently running on {device}')\r\n",
        "\r\n",
        "import spacy\r\n",
        "spacy_en = spacy.load('en')\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import random\r\n",
        "import math\r\n",
        "import time\r\n",
        "import json\r\n",
        "import random\r\n",
        "\r\n",
        "import logging\r\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s : %(levelname)s : %(message)s')\r\n",
        "logger = logging.getLogger('SQUAD-PADDED_ATTN')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently running on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7LjRu4ScquO",
        "outputId": "0cfd7394-1544-4270-d033-938f75e567f3"
      },
      "source": [
        "# Getting the dataset\r\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\r\n",
        "\r\n",
        "# Getting the test dataset\r\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-21 17:26:34--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json.1’\n",
            "\n",
            "train-v2.0.json.1   100%[===================>]  40.17M   228MB/s    in 0.2s    \n",
            "\n",
            "2021-01-21 17:26:34 (228 MB/s) - ‘train-v2.0.json.1’ saved [42123633/42123633]\n",
            "\n",
            "--2021-01-21 17:26:34--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json.1’\n",
            "\n",
            "dev-v2.0.json.1     100%[===================>]   4.17M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-01-21 17:26:34 (110 MB/s) - ‘dev-v2.0.json.1’ saved [4370528/4370528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrA79pVgfHPs"
      },
      "source": [
        "# Setting seeds for reproducability\r\n",
        "\r\n",
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE-xCNyefTOr"
      },
      "source": [
        "## Loading Json and formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR6JT30ZfN5z"
      },
      "source": [
        "with open(\"train-v2.0.json\") as f:\r\n",
        "    train_dict = json.load(f)\r\n",
        "\r\n",
        "with open(\"/content/dev-v2.0.json\") as f:\r\n",
        "    test_dict = json.load(f)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyG6aGaZfkT1"
      },
      "source": [
        "def json_to_df(json_dict):\r\n",
        "    '''\r\n",
        "    Takes input as a dictionary and returns a dataframe of columns InputData and Answer\r\n",
        "    Currently returns the dataframe upto 10k rows due to storage constraints\r\n",
        "    '''\r\n",
        "    df = pd.DataFrame(columns=['InputData', 'Answer'])\r\n",
        "    df_idx = 0\r\n",
        "    for topic in json_dict[\"data\"]:\r\n",
        "        for pgraph in topic[\"paragraphs\"]:\r\n",
        "            \r\n",
        "            for index, qa in enumerate(pgraph[\"qas\"]):\r\n",
        "                if not qa[\"is_impossible\"]:\r\n",
        "                    text = pgraph[\"context\"]\r\n",
        "                    question = qa[\"question\"]\r\n",
        "                    df.at[df_idx, 'InputData'] = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\r\n",
        "                    df.at[df_idx, 'Answer'] = qa[\"answers\"][0]['text']\r\n",
        "                    df_idx += 1\r\n",
        "                \r\n",
        "    return df[:1000]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP8sf2BbfqP8"
      },
      "source": [
        "def get_pandas_data():\r\n",
        "\r\n",
        "    '''Reads the pandas data if already exists'''\r\n",
        "\r\n",
        "    if not os.path.exists('/content/train_data.csv'):\r\n",
        "        train_data = json_to_df(train_dict)\r\n",
        "        test_data = json_to_df(test_dict)\r\n",
        "        train_data.to_csv('train_data.csv', index=False)\r\n",
        "        test_data.to_csv('test_data.csv', index=False)\r\n",
        "    else:\r\n",
        "        train_data = pd.read_csv('/content/train_data.csv')\r\n",
        "        test_data = pd.read_csv('/content/test_data.csv')\r\n",
        "    return train_data, test_data"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDmnVhMeg_xE"
      },
      "source": [
        "train_data, test_data = get_pandas_data()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1h3lwRehB-9",
        "outputId": "6f3cb085-3e76-46fc-bcb8-b573c1734e9b"
      },
      "source": [
        "# Lets see what our output looks like\r\n",
        "print(train_data.head(10))\r\n",
        "print(test_data.head(10))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                           InputData               Answer\n",
            "0  [CLS] When did Beyonce start becoming popular?...    in the late 1990s\n",
            "1  [CLS] What areas did Beyonce compete in when s...  singing and dancing\n",
            "2  [CLS] When did Beyonce leave Destiny's Child a...                 2003\n",
            "3  [CLS] In what city and state did Beyonce  grow...       Houston, Texas\n",
            "4  [CLS] In which decade did Beyonce become famou...           late 1990s\n",
            "5  [CLS] In what R&B group was she the lead singe...      Destiny's Child\n",
            "6  [CLS] What album made her a worldwide known ar...  Dangerously in Love\n",
            "7  [CLS] Who managed the Destiny's Child group? [...       Mathew Knowles\n",
            "8  [CLS] When did Beyoncé rise to fame? [SEP] Bey...           late 1990s\n",
            "9  [CLS] What role did Beyoncé have in Destiny's ...          lead singer\n",
            "                                           InputData                       Answer\n",
            "0  [CLS] In what country is Normandy located? [SE...                       France\n",
            "1  [CLS] When were the Normans in Normandy? [SEP]...      10th and 11th centuries\n",
            "2  [CLS] From which countries did the Norse origi...  Denmark, Iceland and Norway\n",
            "3  [CLS] Who was the Norse leader? [SEP] The Norm...                        Rollo\n",
            "4  [CLS] What century did the Normans first gain ...                 10th century\n",
            "5  [CLS] Who was the duke in the battle of Hastin...        William the Conqueror\n",
            "6  [CLS] Who ruled the duchy of Normandy [SEP] Th...                    Richard I\n",
            "7  [CLS] What religion were the Normans [SEP] The...                     Catholic\n",
            "8  [CLS] What is the original meaning of the word...                       Viking\n",
            "9  [CLS] When was the Latin version of the word N...                  9th century\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxuJPZNQhjtn"
      },
      "source": [
        "## Converting the dataset into processable format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdh_C66PhVi1"
      },
      "source": [
        "def tokenize_en(text):\r\n",
        "    \"\"\"\r\n",
        "    Tokenizes English text from a string into a list of strings\r\n",
        "    \"\"\"\r\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d43fJtUiFAq"
      },
      "source": [
        "### Field is like a tuple that converts the data into SRC and TRG format\r\n",
        "\r\n",
        "Include_lengths = True set for padded sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9-5ym-UiBXI"
      },
      "source": [
        "SRC = Field(tokenize= tokenize_en, \r\n",
        "            init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True,\r\n",
        "            include_lengths = True)\r\n",
        "\r\n",
        "TRG = Field(tokenize = tokenize_en, \r\n",
        "            init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True)\r\n",
        "\r\n",
        "fields = {'InputData': ('q', SRC), 'Answer': ('t', TRG)}"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pggppR_KijbU"
      },
      "source": [
        "train_data, test_data = TabularDataset.splits(\r\n",
        "                                path = '',   \r\n",
        "                                train = 'train_data.csv',\r\n",
        "                                test = 'test_data.csv',\r\n",
        "                                format = 'csv',\r\n",
        "                                fields = fields)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwmx3Wx4inWd"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2, max_size= 10000)\r\n",
        "TRG.build_vocab(train_data, min_freq = 2, max_size= 5000)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uJ0nxd1iyAL"
      },
      "source": [
        "BATCH_SIZE = 24\r\n",
        "\r\n",
        "train_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE,\r\n",
        "    sort=True,\r\n",
        "    sort_key = lambda x: len(x.q),\r\n",
        "    device = device)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl-XzcYlk0Va"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_dim, embedding_dim, enc_hidden_dim, dec_hidden_dim, dropout_size):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # Embedding Hyperparameters\r\n",
        "        # num_embeddings = Size of your input of your vocab\r\n",
        "        # embedding dim = Size of your embeddings dimension\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\r\n",
        "        # From here we send our embeddings to a RNN which then generates output\r\n",
        "\r\n",
        "        # Dropout is used in layers of embeddings and hidden states\r\n",
        "        self.dropout = nn.Dropout(dropout_size)\r\n",
        "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=enc_hidden_dim, bidirectional=True)\r\n",
        "        self.fc = nn.Linear(enc_hidden_dim*2, dec_hidden_dim) # enc_hid_dim * 2 because the nn is bidirectional in nature\r\n",
        "        #Why fc output of fc layer is equal to the dec_hid_dim\r\n",
        "\r\n",
        "    def forward(self, input_src, src_len):\r\n",
        "        # src = [src_len, batch_size]\r\n",
        "        logger.debug('$$$$$$$$$$$$ Encoder Logs Begins $$$$$$$$$$$$$$$')\r\n",
        "        logger.debug(f'Shape of the input dim is [src_len, batch_size] -  {input_src.shape}')\r\n",
        "        embedded_data = self.dropout(self.embedding(input_src))\r\n",
        "        logger.debug(f'Shape of the embedding dim is [src_len, batch_size, embedded_dim] -  {embedded_data.shape}')\r\n",
        "        # embedded = [src_len, batch_size, embedding_dim]\r\n",
        "\r\n",
        "        # src_len = torch.as_tensor([24], dtype=torch.int64)\r\n",
        "        # print('src len=', input_src[0].cpu())\r\n",
        "        # src_len = src_len.unsqueeze(0)\r\n",
        "        logger.debug(f'src_len = {src_len}')\r\n",
        "        packed_embedding = nn.utils.rnn.pack_padded_sequence(embedded_data, src_len.cpu())\r\n",
        "        packed_outputs, hidden = self.rnn(packed_embedding)\r\n",
        "\r\n",
        "        # Add padding back to the output to add gpu processing\r\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\r\n",
        "\r\n",
        "        logger.debug(f'Shape of the output from RNN is [src_len, batch_size, hidden_dim*num_directions] -  {outputs.shape}')\r\n",
        "        logger.debug(f'Shape of the hidden from RNN is [src_len, batch_size, hidden_dim*num_directions] -  {hidden.shape}')\r\n",
        "\r\n",
        "        # output = [src_len, batch_size, hidden_dim*num_directions]\r\n",
        "        # hidden = [n_layers*num_direction, batch_size, hidden_dim]\r\n",
        "        \r\n",
        "\r\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\r\n",
        "\r\n",
        "        logger.debug(f'Shape of the hidden after bidirection concat is -  {hidden.shape}')\r\n",
        "        \r\n",
        "        logger.debug('$$$$$$$$$$$$ Encoder Logs Ends $$$$$$$$$$$$$$$')\r\n",
        "        # hidden_dim = [batch_size, dec hid dim]\r\n",
        "        return outputs, hidden"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CynEFdEe54G5"
      },
      "source": [
        "# Attention mechanism\r\n",
        "\r\n",
        "class Attention(nn.Module):\r\n",
        "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # Attention is basically a dot product between the outputs from the encoder and output from the decoder\r\n",
        "        self.attn = nn.Linear((enc_hidden_dim*2) + dec_hidden_dim, dec_hidden_dim)\r\n",
        "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\r\n",
        "\r\n",
        "    def forward(self, hidden, encoder_outputs, mask):\r\n",
        "        # hidden = [batch_size, dec hid dim]\r\n",
        "        # encoder_outputs = [src_len, batch_size, enc_hid_dim*2]\r\n",
        "        logger.debug('######### Attention Log - Begins#############')\r\n",
        "        logger.debug(f'shape of encoder outputs {encoder_outputs.shape}')\r\n",
        "        batch_size = encoder_outputs.shape[1]\r\n",
        "        src_len = encoder_outputs.shape[0]\r\n",
        "\r\n",
        "        # Repeat decoder hidden state src_len times\r\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\r\n",
        "        logger.debug(f'shape of hidden after unsqueezing source lenght times {hidden.shape}')\r\n",
        "\r\n",
        "        encoder_outputs = encoder_outputs.permute(1,0,2)\r\n",
        "\r\n",
        "        # energy = [batch size, src len, enc_hidden_dim*2]\r\n",
        "        # energy is concat of encoder output \r\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\r\n",
        "\r\n",
        "        logger.debug(f'Shape of energy {energy.shape}')\r\n",
        "\r\n",
        "        attention = self.v(energy).squeeze(2)\r\n",
        "\r\n",
        "        # Fill the attention values of masked fill to very small values\r\n",
        "        logger.debug(f' Shape of attention after squeeze(2) {attention.shape}')\r\n",
        "        attention = attention.masked_fill(mask==0, -1e10)\r\n",
        "\r\n",
        "        logger.debug(f'Shape of attention after masked fill:', attention.shape)\r\n",
        "        # attention = [batch size, src len]\r\n",
        "\r\n",
        "        logger.debug('################## Attention Logs Ends###############################')\r\n",
        "\r\n",
        "        return F.softmax(attention, dim=1) # Get softmax outputs on dim = 1"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AYmer0vqv0s"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, output_dim, embedding_dim,enc_hidden_dim, dec_hidden_dim, dropout_size, attention):\r\n",
        "        super().__init__()\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.attention = attention\r\n",
        "        \r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(num_embeddings=output_dim, embedding_dim=embedding_dim)\r\n",
        "\r\n",
        "        # Output shape of embedding_dim is input_size, embedding_dim\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout_size)\r\n",
        "\r\n",
        "        # Why enc_hidden_dim*2 - Input of attention is going into decoder rnn's along with embedding dim, and previous hidden state\r\n",
        "        self.rnn = nn.GRU(input_size=(enc_hidden_dim*2 + embedding_dim), bidirectional=False, hidden_size=dec_hidden_dim)\r\n",
        "\r\n",
        "        # Output shape of GRU is hidden is of shape - batch_size, input_dim, hidden_dim when unidirectional\r\n",
        "        # Linear layers takes input from embedding layers, attention block, \r\n",
        "\r\n",
        "        self.fc_out = nn.Linear(in_features=(embedding_dim + enc_hidden_dim*2 + dec_hidden_dim), out_features= output_dim)\r\n",
        "\r\n",
        "        # Output shape of linear  layers is concat of all(input, hidden, embedding)\r\n",
        "\r\n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\r\n",
        "\r\n",
        "        # input = [batch_size]\r\n",
        "        # hidden = [batch_size, dec_hidden_dim]\r\n",
        "        # encoder_outputs = [src_len, batch_size, enc_hid_dim*2]\r\n",
        "\r\n",
        "        logger.debug('&&&&&&&&&&&&&&& Decoder Logs Begins &&&&&&&&&&&&&&')\r\n",
        "        logger.debug(f'Input before unsqueeeze {input}, after unsqueeze {input.unsqueeze(0)}')\r\n",
        "\r\n",
        "        input = input.unsqueeze(0)\r\n",
        "        # input = [1, batch_size]\r\n",
        "\r\n",
        "        embedded_data = self.dropout(self.embedding(input))\r\n",
        "        # embedded_data = [1, batch_size, embedding_dim]\r\n",
        "\r\n",
        "        a = self.attention(hidden, encoder_outputs, mask)\r\n",
        "        # attention = [batch_size, src_len]\r\n",
        "\r\n",
        "        # Attention\r\n",
        "        logger.debug('Decoder - attention shape before permute and  unsqueeze(1)', a.shape)    \r\n",
        "        a = a.unsqueeze(1)\r\n",
        "        # attention = [batch size, 1, src_len]\r\n",
        "        logger.debug('Decoder - attention shape after unsqueeze(1)', a.shape)\r\n",
        "\r\n",
        "        # Encoder Outputs\r\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n",
        "        # encoder_outputs = [batch_size, src_len, enc_hid_dim*2]\r\n",
        "\r\n",
        "        # If input is a (b×n×m) tensor, mat2 is a (b×m×p) tensor, out will be a (b×n×p) tensor.\r\n",
        "        logger.debug(f'Attention shape {a.shape} encoder_output shape {encoder_outputs.shape}')\r\n",
        "        weighted = torch.bmm(a, encoder_outputs).permute(1, 0, 2)\r\n",
        "        # weighted = [24, 1, 80] * [24, 80, 1024] = [24, 80, 1024] = [80, 24, 1024]\r\n",
        "        logger.debug(f'shape of weighted - {weighted.shape}')\r\n",
        "\r\n",
        "        outputs, hidden = self.rnn(torch.cat((embedded_data, weighted), dim=2), hidden.unsqueeze(0))\r\n",
        "        # output = [1, batch_size, dec_hid_dim]\r\n",
        "        # hidden = [1, batch_size, dec_hid_dim]\r\n",
        "\r\n",
        "\r\n",
        "        fc_input = torch.cat((embedded_data, outputs, weighted), dim=2)\r\n",
        "        # fc_input = [1, batch_size, emb_dim + dec_hid_dim + enc_hid_dim*2]\r\n",
        "\r\n",
        "        predictions = self.fc_out(fc_input).squeeze(0)\r\n",
        "        # predictions -> [batch_size, output_dim]\r\n",
        "         \r\n",
        "        # Hidden is stacked forwards and backwards\r\n",
        "        logger.debug(f'predictions - shape {predictions.shape}')\r\n",
        "        logger.debug('&&&&&&&&&&&&&&& Decoder Logs Ends &&&&&&&&&&&&&&')\r\n",
        "\r\n",
        "        return predictions, hidden.squeeze(0), a.squeeze(1)\r\n",
        "        # predictions = [batch_size, output_dim]\r\n",
        "        # hidden = [batch_size, dec_hid_dim]\r\n",
        "        # a = [batch_size, src_len]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjIoOlwBwuNL"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder, device, src_pad_idx):\r\n",
        "        super().__init__()\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.device = device\r\n",
        "        self.src_pad_idx =  src_pad_idx\r\n",
        "    \r\n",
        "    def create_mask(self, src):\r\n",
        "        mask = (src != self.src_pad_idx).permute(1,0)\r\n",
        "        return mask\r\n",
        "\r\n",
        "    def forward(self, src, trg, src_len ,teacher_forcing=0.5):\r\n",
        "        # src = [src len, batch_size]\r\n",
        "        # trg = [trg_len, batch_size]\r\n",
        "\r\n",
        "        batch_size = src.shape[1]\r\n",
        "        trg_len = trg.shape[0]\r\n",
        "\r\n",
        "        # Decoder output dim what is?\r\n",
        "        trg_vocab_size = self.decoder.output_dim\r\n",
        "\r\n",
        "        # tensor to store decoder outputs\r\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\r\n",
        "\r\n",
        "        # encoder outputs is all hidden states of the input sequence\r\n",
        "\r\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\r\n",
        "\r\n",
        "        # first input is <sos> token\r\n",
        "        input = trg[0,:]\r\n",
        "        for t in range(1,trg_len):\r\n",
        "            mask = self.create_mask(src)\r\n",
        "            output, hidden, attn = self.decoder(input, hidden, encoder_outputs, mask)\r\n",
        "\r\n",
        "            outputs[t] = output\r\n",
        "\r\n",
        "            teacher_force = random.random() < teacher_forcing\r\n",
        "\r\n",
        "            # Highest predicted token from predictions\r\n",
        "            top1 = output.argmax(1)\r\n",
        "\r\n",
        "            # if teacher forcing, use actual next token as input\r\n",
        "            input = trg[t] if teacher_force else top1\r\n",
        "        return outputs"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf9gbbTNp9Nc"
      },
      "source": [
        "## Training the Seq2Seq model\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkVPnI5fs5IE"
      },
      "source": [
        "\r\n",
        "INPUT_DIM = len(SRC.vocab)\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "ENC_EMB_DIM = 256\r\n",
        "DEC_EMB_DIM = 256\r\n",
        "ENC_HID_DIM = 512\r\n",
        "DEC_HID_DIM = 512\r\n",
        "ENC_DROPOUT = 0.5\r\n",
        "DEC_DROPOUT = 0.5\r\n",
        "\r\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\r\n",
        "\r\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\r\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\r\n",
        "            # input_dim, embedding_dim, enc_hidden_dim, dec_hidden_dim, dropout_size)\r\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\r\n",
        "                # output_dim, embedding_dim,enc_hidden_dim, dec_hidden_dim, dropout_size, attention\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, device, SRC_PAD_IDX).to(device)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_CoOeJxumZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64665885-8fa4-47a0-fd48-8d703f1a21cc"
      },
      "source": [
        "# weights and biases initialized to 0\r\n",
        "\r\n",
        "def init_weights(m):\r\n",
        "    for name, param in m.named_parameters():\r\n",
        "        if 'weight' in name:\r\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\r\n",
        "        else:\r\n",
        "            nn.init.constant_(param.data,0)\r\n",
        "\r\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(2918, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(502, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=502, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2aZF-_ivZZa"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0h6BEkzvayJ"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJwV_nvHveIT"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train() \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "        # print('batch',batch)\r\n",
        "        \r\n",
        "        src, src_len = batch.q\r\n",
        "        trg = batch.t        \r\n",
        "        optimizer.zero_grad()        \r\n",
        "        output = model(src, trg, src_len)\r\n",
        "        \r\n",
        "        #trg = [trg len, batch size]\r\n",
        "        #output = [trg len, batch size, output dim]\r\n",
        "        \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "        \r\n",
        "        output = output[1:].view(-1, output_dim)\r\n",
        "        trg = trg[1:].view(-1)\r\n",
        "        \r\n",
        "        #trg = [(trg len - 1) * batch size]\r\n",
        "        #output = [(trg len - 1) * batch size, output dim]\r\n",
        "        \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfBPoKZlvx7d"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src, src_len = batch.q\r\n",
        "            trg = batch.t\r\n",
        "\r\n",
        "            output = model(src, trg, src_len) #turn off teacher forcing\r\n",
        "\r\n",
        "            #trg = [trg len, batch size]\r\n",
        "            #output = [trg len, batch size, output dim]\r\n",
        "\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output[1:].view(-1, output_dim)\r\n",
        "            trg = trg[1:].view(-1)\r\n",
        "\r\n",
        "            #trg = [(trg len - 1) * batch size]\r\n",
        "            #output = [(trg len - 1) * batch size, output dim]\r\n",
        "\r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfjmGhDGvzdK"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81CNH4yhv38P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dedef80-ab0b-4d67-d520-a142d97e8582"
      },
      "source": [
        "import tqdm\r\n",
        "N_EPOCHS = 20\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    print('Training started')\r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    print('training complete')\r\n",
        "    valid_loss = evaluate(model, test_iterator, criterion)\r\n",
        "    \r\n",
        "\r\n",
        "    end_time = time.time()\r\n",
        "    print(end_time)\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training started\n",
            "training complete\n",
            "1611250053.286074\n",
            "Epoch: 01 | Time: 0m 6s\n",
            "\tTrain Loss: 3.917 | Train PPL:  50.234\n",
            "\t Val. Loss: 1.885 |  Val. PPL:   6.586\n",
            "Training started\n",
            "training complete\n",
            "1611250059.6753592\n",
            "Epoch: 02 | Time: 0m 6s\n",
            "\tTrain Loss: 3.646 | Train PPL:  38.327\n",
            "\t Val. Loss: 2.250 |  Val. PPL:   9.486\n",
            "Training started\n",
            "training complete\n",
            "1611250065.9895358\n",
            "Epoch: 03 | Time: 0m 6s\n",
            "\tTrain Loss: 3.601 | Train PPL:  36.638\n",
            "\t Val. Loss: 2.214 |  Val. PPL:   9.149\n",
            "Training started\n",
            "training complete\n",
            "1611250072.3503861\n",
            "Epoch: 04 | Time: 0m 6s\n",
            "\tTrain Loss: 3.773 | Train PPL:  43.521\n",
            "\t Val. Loss: 2.112 |  Val. PPL:   8.265\n",
            "Training started\n",
            "training complete\n",
            "1611250078.7238147\n",
            "Epoch: 05 | Time: 0m 6s\n",
            "\tTrain Loss: 3.455 | Train PPL:  31.651\n",
            "\t Val. Loss: 2.118 |  Val. PPL:   8.315\n",
            "Training started\n",
            "training complete\n",
            "1611250085.0924964\n",
            "Epoch: 06 | Time: 0m 6s\n",
            "\tTrain Loss: 3.421 | Train PPL:  30.602\n",
            "\t Val. Loss: 2.174 |  Val. PPL:   8.798\n",
            "Training started\n",
            "training complete\n",
            "1611250091.4957643\n",
            "Epoch: 07 | Time: 0m 6s\n",
            "\tTrain Loss: 3.157 | Train PPL:  23.508\n",
            "\t Val. Loss: 2.123 |  Val. PPL:   8.354\n",
            "Training started\n",
            "training complete\n",
            "1611250097.9256227\n",
            "Epoch: 08 | Time: 0m 6s\n",
            "\tTrain Loss: 3.131 | Train PPL:  22.905\n",
            "\t Val. Loss: 2.087 |  Val. PPL:   8.063\n",
            "Training started\n",
            "training complete\n",
            "1611250104.339232\n",
            "Epoch: 09 | Time: 0m 6s\n",
            "\tTrain Loss: 2.929 | Train PPL:  18.704\n",
            "\t Val. Loss: 2.091 |  Val. PPL:   8.096\n",
            "Training started\n",
            "training complete\n",
            "1611250110.7697022\n",
            "Epoch: 10 | Time: 0m 6s\n",
            "\tTrain Loss: 2.976 | Train PPL:  19.607\n",
            "\t Val. Loss: 2.143 |  Val. PPL:   8.524\n",
            "Training started\n",
            "training complete\n",
            "1611250117.1950333\n",
            "Epoch: 11 | Time: 0m 6s\n",
            "\tTrain Loss: 3.072 | Train PPL:  21.575\n",
            "\t Val. Loss: 2.172 |  Val. PPL:   8.776\n",
            "Training started\n",
            "training complete\n",
            "1611250123.6300805\n",
            "Epoch: 12 | Time: 0m 6s\n",
            "\tTrain Loss: 4.803 | Train PPL: 121.862\n",
            "\t Val. Loss: 2.138 |  Val. PPL:   8.485\n",
            "Training started\n",
            "training complete\n",
            "1611250130.056084\n",
            "Epoch: 13 | Time: 0m 6s\n",
            "\tTrain Loss: 3.652 | Train PPL:  38.544\n",
            "\t Val. Loss: 3.428 |  Val. PPL:  30.821\n",
            "Training started\n",
            "training complete\n",
            "1611250136.4938004\n",
            "Epoch: 14 | Time: 0m 6s\n",
            "\tTrain Loss: 3.283 | Train PPL:  26.666\n",
            "\t Val. Loss: 2.497 |  Val. PPL:  12.142\n",
            "Training started\n",
            "training complete\n",
            "1611250142.9540548\n",
            "Epoch: 15 | Time: 0m 6s\n",
            "\tTrain Loss: 3.347 | Train PPL:  28.421\n",
            "\t Val. Loss: 2.440 |  Val. PPL:  11.477\n",
            "Training started\n",
            "training complete\n",
            "1611250149.4100351\n",
            "Epoch: 16 | Time: 0m 6s\n",
            "\tTrain Loss: 4.259 | Train PPL:  70.725\n",
            "\t Val. Loss: 2.320 |  Val. PPL:  10.180\n",
            "Training started\n",
            "training complete\n",
            "1611250155.881719\n",
            "Epoch: 17 | Time: 0m 6s\n",
            "\tTrain Loss: 4.377 | Train PPL:  79.598\n",
            "\t Val. Loss: 2.892 |  Val. PPL:  18.029\n",
            "Training started\n",
            "training complete\n",
            "1611250162.3550816\n",
            "Epoch: 18 | Time: 0m 6s\n",
            "\tTrain Loss: 4.533 | Train PPL:  93.051\n",
            "\t Val. Loss: 2.792 |  Val. PPL:  16.319\n",
            "Training started\n",
            "training complete\n",
            "1611250168.8399339\n",
            "Epoch: 19 | Time: 0m 6s\n",
            "\tTrain Loss: 3.510 | Train PPL:  33.452\n",
            "\t Val. Loss: 2.167 |  Val. PPL:   8.732\n",
            "Training started\n",
            "training complete\n",
            "1611250175.34098\n",
            "Epoch: 20 | Time: 0m 6s\n",
            "\tTrain Loss: 4.132 | Train PPL:  62.318\n",
            "\t Val. Loss: 2.460 |  Val. PPL:  11.700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJJQG11Iv7rE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4368446-0014-4216-e7dd-1887cd9ebdaf"
      },
      "source": [
        "\r\n",
        "model.load_state_dict(torch.load('tut3-model.pt'))\r\n",
        "\r\n",
        "test_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 1.883 | Test PPL:   6.575 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkUeexH6_Jfl"
      },
      "source": [
        "## Inferencing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5EwEybp_Qrd"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib import ticker\r\n",
        "\r\n",
        "def display_attention(question, answer, attention):\r\n",
        "\r\n",
        "  fig = plt.figure(figsize=(10, 10))\r\n",
        "  ax = fig.add_subplot(111)\r\n",
        "\r\n",
        "  attention = attention.squeeze(1).cpu().detach().numpy()\r\n",
        "\r\n",
        "  cax = ax.matshow(attention, cmap='bone')\r\n",
        "\r\n",
        "  ax.tick_params(labelsize=15)\r\n",
        "  ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], rotation=45)\r\n",
        "  ax.set_yticklabels(['']+translation)\r\n",
        "\r\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "  plt.show()\r\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}